{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "DenseNet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajK853/tum-adlr-ss21-11/blob/main/notebook/DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b8e2147-cb53-4429-b77c-0f02bc35475d"
      },
      "source": [
        "# Setup for Google Colab\n",
        "The sections below are required to execute this notebook in Google Colab. If you are not executing this notebook in Google colab, you can skip these sections."
      ],
      "id": "2b8e2147-cb53-4429-b77c-0f02bc35475d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9de8355-1cc5-4089-9b02-adaa3b459bc7"
      },
      "source": [
        "## Load Tensorflow\n",
        "In Google Colab, tensorflow can be easily selected using the given magic command:"
      ],
      "id": "c9de8355-1cc5-4089-9b02-adaa3b459bc7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fffbba7b-ea87-4373-9515-a44979fd1659"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "id": "fffbba7b-ea87-4373-9515-a44979fd1659",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "453cd5d2-6448-4f21-abbd-30c25aa96b9a"
      },
      "source": [
        "## Cloning the repo\n",
        "The shell command below clones the git repo if the local repo directory does not exit. Otherwise, it simply pulls the updated version."
      ],
      "id": "453cd5d2-6448-4f21-abbd-30c25aa96b9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1906f6c4-9c4d-485a-8c2a-7b3b9ff088d5"
      },
      "source": [
        "%%shell\n",
        "REPOSRC=https://github.com/RajK853/tum-adlr-ss21-11.git\n",
        "LOCALREPO=/adlr\n",
        " \n",
        "LOCALREPO_VC_DIR=$LOCALREPO/.git\n",
        " \n",
        "if [ ! -d $LOCALREPO_VC_DIR ]\n",
        "then\n",
        "    git clone $REPOSRC $LOCALREPO\n",
        "else\n",
        "    cd $LOCALREPO\n",
        "    git pull $REPOSRC\n",
        "fi"
      ],
      "id": "1906f6c4-9c4d-485a-8c2a-7b3b9ff088d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2a6fb1-4296-4b68-9b52-bee8ecb5dc30"
      },
      "source": [
        "## Setting the working directory\n",
        "By default, the working directory is `./notebook/` for Jupyter Lab. Therefore, we set the working directory to that path also in Google Colab so that the later cells are compatible with both Google Colab and Jupyter Lab. "
      ],
      "id": "af2a6fb1-4296-4b68-9b52-bee8ecb5dc30"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b234dd3-91f1-444e-8be8-87b7272ee571"
      },
      "source": [
        "%cd /adlr/notebook"
      ],
      "id": "5b234dd3-91f1-444e-8be8-87b7272ee571",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "844709fb-b0b6-41a0-9e52-e5b2bc85553d"
      },
      "source": [
        "## Mount Google Drive\n",
        "Mount your Google drive to access the data set located in your Google drive."
      ],
      "id": "844709fb-b0b6-41a0-9e52-e5b2bc85553d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f04d3084-f7a0-4fe1-8175-fb4d8cf9d054"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "f04d3084-f7a0-4fe1-8175-fb4d8cf9d054",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1241f1e0-4a02-4e05-a840-6187d43e09b3"
      },
      "source": [
        "# Implementation\n",
        "The sections below works for both Google Colab and Jupyter Notebook."
      ],
      "id": "1241f1e0-4a02-4e05-a840-6187d43e09b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e8155e8-bd3e-4bdd-88d6-67f3286c643f"
      },
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.compat.v1.keras import utils\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "0e8155e8-bd3e-4bdd-88d6-67f3286c643f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c67df6c3-6f03-4f03-92ba-696c2fbd4b36"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "id": "c67df6c3-6f03-4f03-92ba-696c2fbd4b36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ea4e752-738b-4eff-b5e1-4de4422e43d2"
      },
      "source": [
        "# Change directory to src to import functions from load.py\n",
        "%cd \"../src\"\n",
        "from models import u_dense_net\n",
        "from generators import DataGen\n",
        "from losses import soft_dice_loss\n",
        "from load import get_values_sql, compressed2img, object2numeric_array"
      ],
      "id": "3ea4e752-738b-4eff-b5e1-4de4422e43d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "077ea3fc-5d01-4b9a-9cef-cba3b489f0ac"
      },
      "source": [
        "# Local functions"
      ],
      "id": "077ea3fc-5d01-4b9a-9cef-cba3b489f0ac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d76c5a4-7961-4a05-ae4f-b49c4795aeb8"
      },
      "source": [
        "def load_world_data(db_path, n_voxels, n_dim):\n",
        "    worlds = get_values_sql(file=db_path, table=\"worlds\")\n",
        "    obstacle_images = compressed2img(img_cmp=worlds.obst_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
        "    return obstacle_images\n",
        "\n",
        "def load_data(db_path, path_indexes, n_voxels, n_dim):    \n",
        "    paths = get_values_sql(file=db_path, table='paths', rows=path_indexes)\n",
        "    path_images = compressed2img(img_cmp=paths.path_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
        "    start_images = compressed2img(img_cmp=paths.start_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
        "    end_images = compressed2img(img_cmp=paths.end_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
        "    \n",
        "    q_paths = object2numeric_array(paths.q_path.values)\n",
        "    q_paths = q_paths.reshape(-1, n_waypoints, n_dim)\n",
        "    return start_images, end_images, path_images\n",
        "\n",
        "def image2image_callback(batch_indexes, data_dict):\n",
        "    path_indexes = data_dict[\"path_rows\"][batch_indexes]\n",
        "    obst_indexes = path_indexes//n_paths_per_world\n",
        "    obst_batch_data = data_dict[\"obst_imgs\"][obst_indexes]\n",
        "    goal_batch_data = data_dict[\"goal_imgs\"][batch_indexes]\n",
        "    path_batch_data = data_dict[\"path_imgs\"][batch_indexes]\n",
        "    input_batch_data = [np.concatenate([obst_batch_data, goal_batch_data], axis=-1)]\n",
        "    output_batch_data = [path_batch_data]\n",
        "    return input_batch_data, output_batch_data"
      ],
      "id": "7d76c5a4-7961-4a05-ae4f-b49c4795aeb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2453b72-efba-43a9-93a8-652fe80faef7"
      },
      "source": [
        "# Init global variables"
      ],
      "id": "b2453b72-efba-43a9-93a8-652fe80faef7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "db5f419e-9ff9-4700-8ab8-d15e1cacfb5c"
      },
      "source": [
        "db_path = r\"/content/drive/MyDrive/ADLR Data Set/SingleSphere02.db\"\n",
        "\n",
        "n_voxels = 64\n",
        "voxel_size = 10 / 64     # in m\n",
        "extent = [0, 10, 0, 10]  # in m\n",
        "n_waypoints = 22         # start + 20 inner points + end\n",
        "n_dim = 2\n",
        "n_paths_per_world = 1000\n",
        "n_paths = 50000\n",
        "path_rows = np.arange(0, n_paths)\n",
        "\n",
        "batch_size = 32\n",
        "validation_ratio = 0.1\n",
        "input_shape = (n_voxels, n_voxels, 2)"
      ],
      "id": "db5f419e-9ff9-4700-8ab8-d15e1cacfb5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a54508-7b49-49dd-9459-12bb399062fc"
      },
      "source": [
        "# Load training and validation data"
      ],
      "id": "55a54508-7b49-49dd-9459-12bb399062fc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15765f3-1c32-454d-a53c-3f08d3d65753"
      },
      "source": [
        "%%time\n",
        "obst_imgs = load_world_data(db_path, n_voxels, n_dim)\n",
        "\n",
        "start_imgs, end_imgs, path_imgs = load_data(db_path, path_rows, n_voxels, n_dim)\n",
        "goal_imgs = start_imgs + end_imgs            # Add start and end images together\n",
        "# Expand dimension of images to create the channel layer\n",
        "obst_imgs, goal_imgs, path_imgs = (\n",
        "    np.expand_dims(data, axis=-1) \n",
        "    for data in (obst_imgs, goal_imgs, path_imgs)\n",
        ")\n",
        "del start_imgs, end_imgs"
      ],
      "id": "f15765f3-1c32-454d-a53c-3f08d3d65753",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5499b78c-b2f1-45e2-b7f9-c2f83b46b982"
      },
      "source": [
        "# TODO: Use better uniform data split technique \n",
        "train_goal_imgs, validation_goal_imgs, train_path_imgs, validation_path_imgs = train_test_split(goal_imgs, path_imgs.astype(\"float32\"), test_size=validation_ratio)\n",
        "train_data_dict = {\n",
        "    \"path_rows\": path_rows,\n",
        "    \"obst_imgs\": obst_imgs,\n",
        "    \"goal_imgs\": train_goal_imgs,\n",
        "    \"path_imgs\": train_path_imgs\n",
        "}\n",
        "\n",
        "validation_data_dict = {\n",
        "    \"path_rows\": path_rows,\n",
        "    \"obst_imgs\": obst_imgs,\n",
        "    \"goal_imgs\": validation_goal_imgs,\n",
        "    \"path_imgs\": validation_path_imgs\n",
        "}"
      ],
      "id": "5499b78c-b2f1-45e2-b7f9-c2f83b46b982",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6d52711-d317-46c2-a1bd-061a7f58e78b"
      },
      "source": [
        "train_data_gen = DataGen(train_data_dict, callback=image2image_callback, batch_size=batch_size)\n",
        "validation_data_gen = DataGen(validation_data_dict, callback=image2image_callback, batch_size=batch_size)"
      ],
      "id": "e6d52711-d317-46c2-a1bd-061a7f58e78b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dd74aa3-7583-442c-964e-01141501f0f1"
      },
      "source": [
        "# Visualize Graph"
      ],
      "id": "0dd74aa3-7583-442c-964e-01141501f0f1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "98e842b4-3746-4b2c-b8af-314c6e1ab401"
      },
      "source": [
        "%cd \"../notebook\"\n",
        "!rm -rf \"temp\"\n",
        "%tensorboard --logdir \"temp\""
      ],
      "id": "98e842b4-3746-4b2c-b8af-314c6e1ab401",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9bac71c-643f-4265-a699-6169402ca9a1"
      },
      "source": [
        "# Load U-DenseNet model"
      ],
      "id": "c9bac71c-643f-4265-a699-6169402ca9a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "70a21437-8b0b-473a-86a5-5e78f2bbbf1b"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "denseNet = u_dense_net(input_shape, num_db=5, num_channels=32, growth_rate=32, convs_per_db=3)\n",
        "optimizer = tf.keras.optimizers.Adam(lr=3e-4)\n",
        "denseNet.compile(optimizer=optimizer, loss=soft_dice_loss)\n",
        "utils.plot_model(denseNet, show_shapes=True)    # Saves model graph as model.png\n",
        "denseNet.summary()"
      ],
      "id": "70a21437-8b0b-473a-86a5-5e78f2bbbf1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8f547a-fd03-4ee9-abf5-8edf2ddc6e1f"
      },
      "source": [
        "# Train U-DenseNet model"
      ],
      "id": "4e8f547a-fd03-4ee9-abf5-8edf2ddc6e1f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "74190ad6-0ad4-4dbc-b1be-eb040ec1b301"
      },
      "source": [
        "%%time\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=\"temp\"),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
        "]\n",
        "history = denseNet.fit(train_data_gen, validation_data=validation_data_gen, epochs=100, callbacks=callbacks)"
      ],
      "id": "74190ad6-0ad4-4dbc-b1be-eb040ec1b301",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca265fb2-9eb9-488d-ac99-f9169bf815c5"
      },
      "source": [
        "# Load testing data"
      ],
      "id": "ca265fb2-9eb9-488d-ac99-f9169bf815c5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8c3d7b2-35ae-4303-8e35-6b94136560f1"
      },
      "source": [
        "test_path_rows = np.arange(n_paths, n_paths+5000, step=100)\n",
        "test_start_imgs, test_end_imgs, test_path_imgs = load_data(db_path, test_path_rows, n_voxels, n_dim)\n",
        "test_goal_imgs = test_start_imgs + test_end_imgs\n",
        "test_goal_imgs, test_path_imgs = (\n",
        "    np.expand_dims(data, axis=-1) \n",
        "    for data in (test_goal_imgs, test_path_imgs)\n",
        ")\n",
        "del test_start_imgs, test_end_imgs"
      ],
      "id": "a8c3d7b2-35ae-4303-8e35-6b94136560f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "70a2f890-ee79-4dfd-8306-0d99d5408481"
      },
      "source": [
        "world_indexes = test_path_rows//n_paths_per_world\n",
        "test_data = np.concatenate([obst_imgs[world_indexes], test_goal_imgs], axis=-1)\n",
        "out_data = denseNet.predict(test_data)"
      ],
      "id": "70a2f890-ee79-4dfd-8306-0d99d5408481",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82cf04b-7919-496b-866d-2325bb93874a"
      },
      "source": [
        "# Visualize on test data"
      ],
      "id": "f82cf04b-7919-496b-866d-2325bb93874a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "3ce80d77-b8a8-4e9f-b042-e9c55645adb5"
      },
      "source": [
        "N = 5\n",
        "path_indexes = np.random.choice(test_path_rows, size=N)\n",
        "_, axs = plt.subplots(nrows=1, ncols=N, figsize=((1+N)*4, 10))\n",
        "for path_i, ax in zip(path_indexes, axs):\n",
        "    i = np.where(test_path_rows==path_i)[0].item()\n",
        "    obst_img = obst_imgs[path_i//n_paths_per_world]\n",
        "    test_goal_img = test_goal_imgs[i]\n",
        "    test_path_img = test_path_imgs[i]\n",
        "\n",
        "    ax.imshow(obst_img[:, :, 0], origin='lower', extent=extent, cmap='binary')\n",
        "    ax.imshow(test_goal_img[:, :, 0], origin='lower', extent=extent, cmap='Greens', alpha=0.25)\n",
        "    ax.imshow(test_path_img[:, :, 0], origin='lower', extent=extent, cmap='Blues', alpha=0.25)\n",
        "    ax.imshow(out_data[i, :, :, 0], origin='lower', extent=extent, cmap='Reds', alpha=0.5)\n",
        "    ax.set_title(f\"Path index: {path_i}, World_index: {path_i//n_paths_per_world}\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "plt.show()"
      ],
      "id": "3ce80d77-b8a8-4e9f-b042-e9c55645adb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b1c25a4-f468-4b5d-ac2f-9d95caadb0cd"
      },
      "source": [
        "# TODOs\n",
        "1. Save trained model\n",
        "2. Save prediction images from intermediate models"
      ],
      "id": "7b1c25a4-f468-4b5d-ac2f-9d95caadb0cd"
    }
  ]
}